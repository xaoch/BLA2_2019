{
  "cells": [
    {
      "metadata": {
        "collapsed": true
      },
      "cell_type": "markdown",
      "source": "# Multimodal Learning Analytics Assignments\n\nAs discussed in classroom, you will be given the features extracted from the raw recordings.  It is up to you to create any type of report that help us understand better the collaboration.\n\nHere is the data:\n\n## OpenPose - Skeleton\n\nThe video was feed to the <a href=\"https://github.com/CMU-Perceptual-Computing-Lab/openpose\">OpenPose</a> library to extract the skeleton of body landmarks of the participant.\n\nThe data was originally in json, but it was converted into a csv file for your convinience.  It only contains the 1000 first video frames (that is almost 67 seconds of the video).\n\nLet's load it:"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "import pandas as pd\n\nskeletons= pd.read_csv(\"openPose.csv\")\nskeletons.columns",
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 5,
          "data": {
            "text/plain": "Index(['frame', 'Nose_x', 'Nose_y', 'Nose_c', 'Neck_x', 'Neck_y', 'Neck_c',\n       'RShoulder_x', 'Shoulder_y', 'RShoulder_c', 'RElbow_x', 'RElbow_y',\n       'RElbow_c', 'RWrist_x', 'RWrist_y', 'RWrist_c', 'LShoulder_x',\n       'LShoulder_y', 'LShoulder_c', 'LElbow_x', 'LElbow_y', 'LElbow_c',\n       'LWrist_x', 'LWrist_y', 'LWrist_c', 'MidHip_x', 'MidHip_y', 'MidHip_c',\n       'RHip_x', 'RHip_y', 'RHip_c', 'RKnee_x', 'RKnee_y', 'RKnee_c',\n       'RAnkle_x', 'RAnkle_y', 'RAnkle_c', 'LHip_x', 'LHip_y', 'LHip_c',\n       'LKnee_x', 'LKnee_y', 'LKnee_c', 'LAnkle_x', 'LAnkle_y', 'LAnkle_c',\n       'REye_x', 'REye_y', 'REye_c', 'LEye_x', 'LEye_y', 'LEye_c', 'REar_x',\n       'REar_y', 'REar_c', 'LEar_x', 'LEar_y', 'LEar_c', 'LBigToe_x',\n       'LBigToe_y', 'LBigToe_c', 'LSmallToe_x', 'LSmallToe_y', 'LSmallToe_c',\n       'LHeel_x', 'LHeel_y', 'LHeel_c', 'RBigToe_x', 'RBigToe_y', 'RBigToe_c',\n       'RSmallToe_x', 'RSmallToe_y', 'RSmallToe_c', 'RHeel_x', 'RHeel_y',\n       'RHeel_c'],\n      dtype='object')"
          },
          "metadata": {}
        }
      ]
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Each row in the datasframe contains the frame number (first column) followed by the position and confidence of each one of the body landmarks.  These body landmarks can be seen in the columns names.  'x' and 'y' are the coordinates of that landmark and 'c' is the confidence of the detection."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "skeletons.columns",
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 6,
          "data": {
            "text/plain": "Index(['frame', 'Nose_x', 'Nose_y', 'Nose_c', 'Neck_x', 'Neck_y', 'Neck_c',\n       'RShoulder_x', 'Shoulder_y', 'RShoulder_c', 'RElbow_x', 'RElbow_y',\n       'RElbow_c', 'RWrist_x', 'RWrist_y', 'RWrist_c', 'LShoulder_x',\n       'LShoulder_y', 'LShoulder_c', 'LElbow_x', 'LElbow_y', 'LElbow_c',\n       'LWrist_x', 'LWrist_y', 'LWrist_c', 'MidHip_x', 'MidHip_y', 'MidHip_c',\n       'RHip_x', 'RHip_y', 'RHip_c', 'RKnee_x', 'RKnee_y', 'RKnee_c',\n       'RAnkle_x', 'RAnkle_y', 'RAnkle_c', 'LHip_x', 'LHip_y', 'LHip_c',\n       'LKnee_x', 'LKnee_y', 'LKnee_c', 'LAnkle_x', 'LAnkle_y', 'LAnkle_c',\n       'REye_x', 'REye_y', 'REye_c', 'LEye_x', 'LEye_y', 'LEye_c', 'REar_x',\n       'REar_y', 'REar_c', 'LEar_x', 'LEar_y', 'LEar_c', 'LBigToe_x',\n       'LBigToe_y', 'LBigToe_c', 'LSmallToe_x', 'LSmallToe_y', 'LSmallToe_c',\n       'LHeel_x', 'LHeel_y', 'LHeel_c', 'RBigToe_x', 'RBigToe_y', 'RBigToe_c',\n       'RSmallToe_x', 'RSmallToe_y', 'RSmallToe_c', 'RHeel_x', 'RHeel_y',\n       'RHeel_c'],\n      dtype='object')"
          },
          "metadata": {}
        }
      ]
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## OpenFace - Face Landmarks and Action Units\n\nThen we extract the facial features from each frame using the <a href=\"https://github.com/TadasBaltrusaitis/OpenFace\">OpenFace library</a>. \n\nThe format of the CSV is explained in this <a href=\"https://github.com/TadasBaltrusaitis/OpenFace/wiki/Output-Format\">webpage</a>.  "
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "faces = pd.read_csv('openFace.csv')\nfaces.head()",
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 7,
          "data": {
            "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Unnamed: 0</th>\n      <th>frame</th>\n      <th>face_id</th>\n      <th>timestamp</th>\n      <th>confidence</th>\n      <th>success</th>\n      <th>gaze_0_x</th>\n      <th>gaze_0_y</th>\n      <th>gaze_0_z</th>\n      <th>gaze_1_x</th>\n      <th>...</th>\n      <th>AU12_c</th>\n      <th>AU14_c</th>\n      <th>AU15_c</th>\n      <th>AU17_c</th>\n      <th>AU20_c</th>\n      <th>AU23_c</th>\n      <th>AU25_c</th>\n      <th>AU26_c</th>\n      <th>AU28_c</th>\n      <th>AU45_c</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0.000</td>\n      <td>0.98</td>\n      <td>1</td>\n      <td>0.381562</td>\n      <td>0.025194</td>\n      <td>-0.924000</td>\n      <td>0.287852</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0.000</td>\n      <td>0.88</td>\n      <td>1</td>\n      <td>-0.244583</td>\n      <td>-0.097918</td>\n      <td>-0.964672</td>\n      <td>-0.378948</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2</td>\n      <td>1</td>\n      <td>2</td>\n      <td>0.000</td>\n      <td>0.98</td>\n      <td>1</td>\n      <td>-0.278326</td>\n      <td>0.092222</td>\n      <td>-0.956049</td>\n      <td>-0.408839</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>3</td>\n      <td>1</td>\n      <td>3</td>\n      <td>0.000</td>\n      <td>0.88</td>\n      <td>1</td>\n      <td>-0.509604</td>\n      <td>-0.072266</td>\n      <td>-0.857369</td>\n      <td>-0.600042</td>\n      <td>...</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>4</td>\n      <td>2</td>\n      <td>0</td>\n      <td>0.067</td>\n      <td>0.98</td>\n      <td>1</td>\n      <td>0.354185</td>\n      <td>0.053495</td>\n      <td>-0.933644</td>\n      <td>0.262728</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows Ã— 715 columns</p>\n</div>",
            "text/plain": "   Unnamed: 0  frame   face_id   timestamp   confidence   success   gaze_0_x  \\\n0           0      1         0       0.000         0.98         1   0.381562   \n1           1      1         1       0.000         0.88         1  -0.244583   \n2           2      1         2       0.000         0.98         1  -0.278326   \n3           3      1         3       0.000         0.88         1  -0.509604   \n4           4      2         0       0.067         0.98         1   0.354185   \n\n    gaze_0_y   gaze_0_z   gaze_1_x   ...      AU12_c   AU14_c   AU15_c  \\\n0   0.025194  -0.924000   0.287852   ...         0.0      0.0      0.0   \n1  -0.097918  -0.964672  -0.378948   ...         0.0      1.0      1.0   \n2   0.092222  -0.956049  -0.408839   ...         0.0      1.0      1.0   \n3  -0.072266  -0.857369  -0.600042   ...         1.0      0.0      1.0   \n4   0.053495  -0.933644   0.262728   ...         0.0      0.0      1.0   \n\n    AU17_c   AU20_c   AU23_c   AU25_c   AU26_c   AU28_c   AU45_c  \n0      0.0      0.0      0.0      1.0      1.0      1.0      1.0  \n1      0.0      1.0      1.0      0.0      0.0      0.0      1.0  \n2      0.0      1.0      1.0      0.0      0.0      1.0      0.0  \n3      0.0      1.0      0.0      1.0      0.0      0.0      1.0  \n4      0.0      0.0      0.0      1.0      1.0      1.0      1.0  \n\n[5 rows x 715 columns]"
          },
          "metadata": {}
        }
      ]
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "The most important features extracted are:\n\ngaze_angle_x, gaze_angle_y Eye gaze direction in radians in world coordinates averaged for both eyes and converted into more easy to use format than gaze vectors. If a person is looking left-right this will results in the change of gaze_angle_x (from positive to negative) and, if a person is looking up-down this will result in change of gaze_angle_y (from negative to positive), if a person is looking straight ahead both of the angles will be close to 0 (within measurement error).\n\npose_Tx, pose_Ty, pose_Tz the location of the head with respect to camera in millimeters (positive Z is away from the camera)\n\npose_Rx, pose_Ry, pose_Rz Rotation is in radians around X,Y,Z axes with the convention R = Rx * Ry * Rz, left-handed positive sign. This can be seen as pitch (Rx), yaw (Ry), and roll (Rz). The rotation is in world coordinates with camera being the origin."
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## Direction of Arrival - DOA\n\nNow we will get from the audio the direction of arrival of the sound.  This is an indicator of who is talking.\n\nEach audio frame is equal to 3 video frames.  A conversion needs to be done to synch the two signals.\n\nEach line in the data correspond to the angle from which the audio was comming."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "doa = pd.read_csv('doa.csv')\ndoa.head()",
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 11,
          "data": {
            "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>frame</th>\n      <th>doa</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>[62.]</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1</td>\n      <td>[61.]</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2</td>\n      <td>[59.]</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>3</td>\n      <td>[58.]</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>4</td>\n      <td>[58.]</td>\n    </tr>\n  </tbody>\n</table>\n</div>",
            "text/plain": "   frame    doa\n0      0  [62.]\n1      1  [61.]\n2      2  [59.]\n3      3  [58.]\n4      4  [58.]"
          },
          "metadata": {}
        }
      ]
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "# Words being said\n\nWe also have an automatic transcript of all the words being said.\n\nIn the data we have the following columns:\n\n* type:  if it is text or punctuation\n* value:  word or puntuation sign\n* start: time at which the word started (only for type text)\n* end:  time at which the word ended (only for type text)\n* confidence: probability that the word is correct\n"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "words = pd.read_csv('words.csv')\nwords.head()",
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 10,
          "data": {
            "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>type</th>\n      <th>value</th>\n      <th>start</th>\n      <th>end</th>\n      <th>confidence</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>text</td>\n      <td>Physical</td>\n      <td>0.24</td>\n      <td>0.72</td>\n      <td>1.00</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>punct</td>\n      <td></td>\n      <td>0.00</td>\n      <td>0.00</td>\n      <td>0.00</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>text</td>\n      <td>assessments</td>\n      <td>0.72</td>\n      <td>1.32</td>\n      <td>0.95</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>punct</td>\n      <td>.</td>\n      <td>0.00</td>\n      <td>0.00</td>\n      <td>0.00</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>punct</td>\n      <td></td>\n      <td>0.00</td>\n      <td>0.00</td>\n      <td>0.00</td>\n    </tr>\n  </tbody>\n</table>\n</div>",
            "text/plain": "    type        value  start   end  confidence\n0   text     Physical   0.24  0.72        1.00\n1  punct                0.00  0.00        0.00\n2   text  assessments   0.72  1.32        0.95\n3  punct            .   0.00  0.00        0.00\n4  punct                0.00  0.00        0.00"
          },
          "metadata": {}
        }
      ]
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## Useful functions to mix features\n\n### Getting the person out of the skeleton position"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "def identifyPerson(positon_X):\n    posX=position_X\n    if posX<400:\n        return \"A\"\n    if posX>399 and posX<800:\n        return \"B\"\n    if posX>799 and posX<1200:\n        return \"C\"\n    if posX>1119 and posX<1500:\n        return \"D\"\n    if posX>1499:\n        return \"E\"",
      "execution_count": 14,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "for skeleton in skeletons.iterrows():\n    #print(\"Frame:\", skeleton[\"frame\"],\"- Positon:\",skeleton[\"Neck_x\"],\"- Identity\", identifyPerson(skeleton[\"Neck_x\"]))\n    print(skeleton[\"Neck_x\"])",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python36",
      "display_name": "Python 3.6",
      "language": "python"
    },
    "language_info": {
      "mimetype": "text/x-python",
      "nbconvert_exporter": "python",
      "name": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.6",
      "file_extension": ".py",
      "codemirror_mode": {
        "version": 3,
        "name": "ipython"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}